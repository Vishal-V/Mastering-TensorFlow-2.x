{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow 2.0 Practice",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFfbyRVDnUf4",
        "colab_type": "code",
        "outputId": "0a46654e-ca91-4b10-ec38-788aa2f4c168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "! pip -q install tensorflow==2.0.0.alpha0\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-alpha0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMsVjQT0nqM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzhuJhiSTl5W",
        "colab_type": "code",
        "outputId": "34dc885d-89b7-432a-b641-54cddb3418b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "save_fig(\"keras_learning_curves_plot\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-e5ab1d7e7f53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msave_fig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keras_learning_curves_plot\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_fig' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAEzCAYAAAALosttAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSVJm/j8N0tvrtDIstQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bfmho1W2uqv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = fetch_california_housing()\n",
        "\n",
        "X_train_full, X_test, Y_train_full, Y_test = train_test_split(data.data, data.target)\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train_full, Y_train_full)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQuOK9W48AXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "X_val = scaler.transform(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp-IyXWe3TGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
        "layer1 = tf.keras.layers.Dense(30, activation='relu')(input)\n",
        "layer2 = tf.keras.layers.Dense(30, activation='relu')(layer1)\n",
        "wide = tf.keras.layers.Concatenate(axis=1)([input, layer2])\n",
        "outputs = tf.keras.layers.Dense(1)(wide)\n",
        "model = tf.keras.models.Model(inputs=[input], outputs=[outputs])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_IIiTd4_7L2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5ynMSp289KT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='sgd', loss='mean_squared_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpTWvqSd__p1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import History\n",
        "history = History()\n",
        "model.fit(X_train, Y_train, batch_size=32, epochs=20, validation_data=(X_val, Y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W52UNpOTAP6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiURIA95CzwJ",
        "colab_type": "text"
      },
      "source": [
        "### Subclassing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7OCIDoaCnw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.models.Model):\n",
        "    def __init__(self, **kwargs):\n",
        "        super(MyModel, self).__init__(**kwargs)\n",
        "        self.hidden1 = tf.keras.layers.Dense(30, activation='relu')\n",
        "        self.hidden2 = tf.keras.layers.Dense(30, activation='relu')\n",
        "        self.concat = tf.keras.layers.Concatenate(axis=1)\n",
        "        self.outputs = tf.keras.layers.Dense(1)\n",
        "        \n",
        "    def call(self, input):\n",
        "        x = self.hidden1(input)\n",
        "        x = self.hidden2(x)\n",
        "        x = self.concat(axis=1)([input, x])\n",
        "        return self.outputs(x)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVm5-wT6GlM_",
        "colab_type": "code",
        "outputId": "ce18efdf-c73c-4353-bf4b-e63a60c3777b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        }
      },
      "source": [
        "model1 = MyModel()\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_11 (InputLayer)           [(None, 8)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense_27 (Dense)                (None, 30)           270         input_11[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_28 (Dense)                (None, 30)           930         dense_27[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_8 (Concatenate)     (None, 38)           0           input_11[0][0]                   \n",
            "                                                                 dense_28[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_29 (Dense)                (None, 1)            39          concatenate_8[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 1,239\n",
            "Trainable params: 1,239\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvByuuN4GpMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model1.compile(optimizer=tf.keras.optimizers.Adam(0.1), loss='mean_squared_error')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1hLGu7RHKxF",
        "colab_type": "code",
        "outputId": "2d520cd4-259f-4813-9d99-d5ae1e21f8fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "history = model.fit(X_train, Y_train, epochs=20, validation_data=(X_val, Y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3129 - val_loss: 30029.5750\n",
            "Epoch 2/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3124 - val_loss: 27302.1298\n",
            "Epoch 3/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3130 - val_loss: 33128.6907\n",
            "Epoch 4/20\n",
            "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3124 - val_loss: 30312.5101\n",
            "Epoch 5/20\n",
            "11610/11610 [==============================] - 0s 38us/sample - loss: 0.3116 - val_loss: 29377.6638\n",
            "Epoch 6/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3115 - val_loss: 28954.2183\n",
            "Epoch 7/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3110 - val_loss: 26030.6216\n",
            "Epoch 8/20\n",
            "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3112 - val_loss: 30604.8349\n",
            "Epoch 9/20\n",
            "11610/11610 [==============================] - 0s 38us/sample - loss: 0.3106 - val_loss: 28396.6534\n",
            "Epoch 10/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3110 - val_loss: 26740.3666\n",
            "Epoch 11/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3099 - val_loss: 24422.6560\n",
            "Epoch 12/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3101 - val_loss: 25714.9720\n",
            "Epoch 13/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3101 - val_loss: 25546.3225\n",
            "Epoch 14/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3094 - val_loss: 26760.5092\n",
            "Epoch 15/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3100 - val_loss: 25399.4419\n",
            "Epoch 16/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3085 - val_loss: 27303.0847\n",
            "Epoch 17/20\n",
            "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3087 - val_loss: 24499.9855\n",
            "Epoch 18/20\n",
            "11610/11610 [==============================] - 0s 38us/sample - loss: 0.3081 - val_loss: 26105.9574\n",
            "Epoch 19/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3105 - val_loss: 26059.9253\n",
            "Epoch 20/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3086 - val_loss: 24234.2166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nPKigv-HfHH",
        "colab_type": "code",
        "outputId": "f2fd4e20-15a7-46ff-cf8a-14f25a4fd1ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.evaluate(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5160/5160 [==============================] - 0s 20us/sample - loss: 0.3260\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.32602607404538825"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2w9egO9TOyJ",
        "colab_type": "code",
        "outputId": "35762bd5-dd76-48d3-c6b4-aba4a0386ca7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
        "plt.grid(True)\n",
        "plt.gca().set_ylim(0, 1)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAEzCAYAAAALosttAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJCkzhrMkSZn5/1Zw3u0uEQN6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwXT7015XGMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "log_dir = os.path.join(os.curdir, \"Samplelogs\")\n",
        "\n",
        "def get_log_dir():\n",
        "    import time\n",
        "    times = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
        "    return os.path.join(log_dir, times)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJu3WMRehGk4",
        "colab_type": "code",
        "outputId": "97c13108-148f-42db-a4d4-8c3746d5aecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        }
      },
      "source": [
        "event_dir = get_log_dir()\n",
        "tb_callback = tf.keras.callbacks.TensorBoard(event_dir)\n",
        "\n",
        "history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=20, callbacks=[tb_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3078 - val_loss: 24626.3002\n",
            "Epoch 2/20\n",
            "11610/11610 [==============================] - 1s 44us/sample - loss: 0.3072 - val_loss: 27169.2507\n",
            "Epoch 3/20\n",
            "11610/11610 [==============================] - 0s 43us/sample - loss: 0.3075 - val_loss: 24868.6192\n",
            "Epoch 4/20\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3069 - val_loss: 23068.6880\n",
            "Epoch 5/20\n",
            "11610/11610 [==============================] - 1s 47us/sample - loss: 0.3067 - val_loss: 24830.5081\n",
            "Epoch 6/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3065 - val_loss: 25137.2094\n",
            "Epoch 7/20\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3061 - val_loss: 25897.8877\n",
            "Epoch 8/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3066 - val_loss: 24050.0588\n",
            "Epoch 9/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3060 - val_loss: 21905.5265\n",
            "Epoch 10/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3053 - val_loss: 18237.9282\n",
            "Epoch 11/20\n",
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.3053 - val_loss: 26889.0307\n",
            "Epoch 12/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3059 - val_loss: 21054.5100\n",
            "Epoch 13/20\n",
            "11610/11610 [==============================] - 1s 46us/sample - loss: 0.3049 - val_loss: 23941.1957\n",
            "Epoch 14/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3046 - val_loss: 24177.0292\n",
            "Epoch 15/20\n",
            "11610/11610 [==============================] - 0s 37us/sample - loss: 0.3048 - val_loss: 19544.1341\n",
            "Epoch 16/20\n",
            "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3056 - val_loss: 19331.6634\n",
            "Epoch 17/20\n",
            "11610/11610 [==============================] - 0s 38us/sample - loss: 0.3043 - val_loss: 23539.8984\n",
            "Epoch 18/20\n",
            "11610/11610 [==============================] - 0s 36us/sample - loss: 0.3042 - val_loss: 26273.3137\n",
            "Epoch 19/20\n",
            "11610/11610 [==============================] - 0s 38us/sample - loss: 0.3038 - val_loss: 20251.6082\n",
            "Epoch 20/20\n",
            "11610/11610 [==============================] - 0s 39us/sample - loss: 0.3042 - val_loss: 19627.9435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYCh89BNhfbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tb(logdir=log_dir, port=6006, open_tab=True, sleep=3):\n",
        "    import subprocess\n",
        "    proc = subprocess.Popen(\n",
        "        \"tensorboard --logdir={0} --port={1}\".format(logdir, port), shell=True)\n",
        "    if open_tab:\n",
        "        import time\n",
        "        print(\"Waiting a few seconds for the TensorBoard Server to start...\")\n",
        "        time.sleep(sleep)\n",
        "        import webbrowser\n",
        "        webbrowser.open(\"http://127.0.0.1:{}/\".format(port))\n",
        "    return proc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVYQogIsicee",
        "colab_type": "code",
        "outputId": "27000799-34ec-4378-f6c9-3d5e6301c206",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3803
        }
      },
      "source": [
        "help(tf.keras.callbacks.TensorBoard)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Help on class TensorBoard in module tensorflow.python.keras.callbacks:\n",
            "\n",
            "class TensorBoard(Callback)\n",
            " |  Enable visualizations for TensorBoard.\n",
            " |  \n",
            " |  TensorBoard is a visualization tool provided with TensorFlow.\n",
            " |  \n",
            " |  This callback logs events for TensorBoard, including:\n",
            " |  * Metrics summary plots\n",
            " |  * Training graph visualization\n",
            " |  * Activation histograms\n",
            " |  * Sampled profiling\n",
            " |  \n",
            " |  If you have installed TensorFlow with pip, you should be able\n",
            " |  to launch TensorBoard from the command line:\n",
            " |  \n",
            " |  ```sh\n",
            " |  tensorboard --logdir=path_to_your_logs\n",
            " |  ```\n",
            " |  \n",
            " |  You can find more information about TensorBoard\n",
            " |  [here](https://www.tensorflow.org/get_started/summaries_and_tensorboard).\n",
            " |  \n",
            " |  Arguments:\n",
            " |      log_dir: the path of the directory where to save the log files to be\n",
            " |        parsed by TensorBoard.\n",
            " |      histogram_freq: frequency (in epochs) at which to compute activation and\n",
            " |        weight histograms for the layers of the model. If set to 0, histograms\n",
            " |        won't be computed. Validation data (or split) must be specified for\n",
            " |        histogram visualizations.\n",
            " |      write_graph: whether to visualize the graph in TensorBoard. The log file\n",
            " |        can become quite large when write_graph is set to True.\n",
            " |      write_images: whether to write model weights to visualize as image in\n",
            " |        TensorBoard.\n",
            " |      update_freq: `'batch'` or `'epoch'` or integer. When using `'batch'`,\n",
            " |        writes the losses and metrics to TensorBoard after each batch. The same\n",
            " |        applies for `'epoch'`. If using an integer, let's say `1000`, the\n",
            " |        callback will write the metrics and losses to TensorBoard every 1000\n",
            " |        samples. Note that writing too frequently to TensorBoard can slow down\n",
            " |        your training.\n",
            " |      profile_batch: Profile the batch to sample compute characteristics. By\n",
            " |        default, it will profile the second batch. Set profile_batch=0 to\n",
            " |        disable profiling. Must run in TensorFlow eager mode.\n",
            " |  \n",
            " |  Raises:\n",
            " |      ValueError: If histogram_freq is set and no validation data is provided.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      TensorBoard\n",
            " |      Callback\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, log_dir='logs', histogram_freq=0, write_graph=True, write_images=False, update_freq='epoch', profile_batch=2, **kwargs)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  on_batch_end(self, batch, logs=None)\n",
            " |      Writes scalar summaries for metrics on every training batch.\n",
            " |      \n",
            " |      Performs profiling if current batch is in profiler_batches.\n",
            " |  \n",
            " |  on_epoch_end(self, epoch, logs=None)\n",
            " |      Runs metrics and histogram summaries at epoch end.\n",
            " |  \n",
            " |  on_train_begin(self, logs=None)\n",
            " |      Called at the beginning of training.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_train_end(self, logs=None)\n",
            " |      Called at the end of training.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  set_model(self, model)\n",
            " |      Sets Keras model and writes graph if specified.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from Callback:\n",
            " |  \n",
            " |  on_batch_begin(self, batch, logs=None)\n",
            " |      A backwards compatibility alias for `on_train_batch_begin`.\n",
            " |  \n",
            " |  on_epoch_begin(self, epoch, logs=None)\n",
            " |      Called at the start of an epoch.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run. This function should only\n",
            " |      be called during TRAIN mode.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          epoch: integer, index of epoch.\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_predict_batch_begin(self, batch, logs=None)\n",
            " |      Called at the beginning of a batch in `predict` methods.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
            " |            number and the size of the batch.\n",
            " |  \n",
            " |  on_predict_batch_end(self, batch, logs=None)\n",
            " |      Called at the end of a batch in `predict` methods.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Metric results for this batch.\n",
            " |  \n",
            " |  on_predict_begin(self, logs=None)\n",
            " |      Called at the beginning of prediction.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_predict_end(self, logs=None)\n",
            " |      Called at the end of prediction.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_test_batch_begin(self, batch, logs=None)\n",
            " |      Called at the beginning of a batch in `evaluate` methods.\n",
            " |      \n",
            " |      Also called at the beginning of a validation batch in the `fit`\n",
            " |      methods, if validation data is provided.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
            " |            number and the size of the batch.\n",
            " |  \n",
            " |  on_test_batch_end(self, batch, logs=None)\n",
            " |      Called at the end of a batch in `evaluate` methods.\n",
            " |      \n",
            " |      Also called at the end of a validation batch in the `fit`\n",
            " |      methods, if validation data is provided.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Metric results for this batch.\n",
            " |  \n",
            " |  on_test_begin(self, logs=None)\n",
            " |      Called at the beginning of evaluation or validation.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_test_end(self, logs=None)\n",
            " |      Called at the end of evaluation or validation.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          logs: dict. Currently no data is passed to this argument for this method\n",
            " |            but that may change in the future.\n",
            " |  \n",
            " |  on_train_batch_begin(self, batch, logs=None)\n",
            " |      Called at the beginning of a training batch in `fit` methods.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Has keys `batch` and `size` representing the current batch\n",
            " |            number and the size of the batch.\n",
            " |  \n",
            " |  on_train_batch_end(self, batch, logs=None)\n",
            " |      Called at the end of a training batch in `fit` methods.\n",
            " |      \n",
            " |      Subclasses should override for any actions to run.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          batch: integer, index of batch within the current epoch.\n",
            " |          logs: dict. Metric results for this batch.\n",
            " |  \n",
            " |  set_params(self, params)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from Callback:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hjt4sTaqSTH",
        "colab_type": "text"
      },
      "source": [
        "HyperParameter Tuning\n",
        "- Hyperopt: a popular Python library for optimizing over all sorts of complex\n",
        "search spaces (including real values such as the learning rate, or discrete values\n",
        "such as the number of layers).\n",
        "- Hyperas, kopt or Talos: optimizing hyperparameters for Keras model (the first\n",
        "two are based on Hyperopt).\n",
        "- Scikit-Optimize (skopt): a general-purpose optimization library. The Bayes\n",
        "SearchCV class performs Bayesian optimization using an interface similar to Grid\n",
        "SearchCV .\n",
        "- Spearmint: a Bayesian optimization library.\n",
        "- Sklearn-Deap: a hyperparameter optimization library based on evolutionary\n",
        "algorithms, also with a GridSearchCV -like interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-uumutLj0Wo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
        "    model = tf.keras.models.Sequential()\n",
        "    options = {\"input_shape\": input_shape}\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\", **options))\n",
        "        options = {}\n",
        "    model.add(tf.keras.layers.Dense(1, **options))\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
        "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQZoDmB7qyfR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "keras_reg = tf.keras.wrappers.scikit_learn.KerasRegressor(build_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfdHL-x9rE1c",
        "colab_type": "code",
        "outputId": "f62f9cd4-4ee1-412a-9ec8-30ea0be912df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3607
        }
      },
      "source": [
        "keras_reg.fit(X_train, Y_train, epochs=100, validation_data=(X_val, Y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 11610 samples, validate on 3870 samples\n",
            "Epoch 1/100\n",
            "11610/11610 [==============================] - 1s 51us/sample - loss: 1.1905 - val_loss: 22188.1083\n",
            "Epoch 2/100\n",
            "11610/11610 [==============================] - 1s 43us/sample - loss: 0.9192 - val_loss: 229160.0355\n",
            "Epoch 3/100\n",
            "11610/11610 [==============================] - 0s 42us/sample - loss: 0.5423 - val_loss: 109126.5876\n",
            "Epoch 4/100\n",
            "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4955 - val_loss: 59602.6433\n",
            "Epoch 5/100\n",
            "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4771 - val_loss: 51414.3624\n",
            "Epoch 6/100\n",
            "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4602 - val_loss: 41412.2395\n",
            "Epoch 7/100\n",
            "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4508 - val_loss: 53116.3814\n",
            "Epoch 8/100\n",
            "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4444 - val_loss: 41733.9572\n",
            "Epoch 9/100\n",
            "11610/11610 [==============================] - 0s 41us/sample - loss: 0.4339 - val_loss: 48842.8379\n",
            "Epoch 10/100\n",
            "11610/11610 [==============================] - 0s 42us/sample - loss: 0.4272 - val_loss: 52302.7520\n",
            "Epoch 11/100\n",
            "11610/11610 [==============================] - 0s 38us/sample - loss: 0.4226 - val_loss: 46329.0479\n",
            "Epoch 12/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.4173 - val_loss: 52719.6499\n",
            "Epoch 13/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.4148 - val_loss: 39615.9418\n",
            "Epoch 14/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4105 - val_loss: 44659.4886\n",
            "Epoch 15/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.4078 - val_loss: 43900.7518\n",
            "Epoch 16/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4049 - val_loss: 49579.2795\n",
            "Epoch 17/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4027 - val_loss: 37315.8565\n",
            "Epoch 18/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.4004 - val_loss: 46364.4217\n",
            "Epoch 19/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3982 - val_loss: 45288.7416\n",
            "Epoch 20/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3962 - val_loss: 44168.7615\n",
            "Epoch 21/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3948 - val_loss: 41787.1750\n",
            "Epoch 22/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3924 - val_loss: 54299.0821\n",
            "Epoch 23/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3908 - val_loss: 37861.2568\n",
            "Epoch 24/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3891 - val_loss: 30778.4784\n",
            "Epoch 25/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3877 - val_loss: 33382.6925\n",
            "Epoch 26/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3857 - val_loss: 33448.5242\n",
            "Epoch 27/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3838 - val_loss: 39433.9755\n",
            "Epoch 28/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3831 - val_loss: 30406.0609\n",
            "Epoch 29/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3811 - val_loss: 22302.9523\n",
            "Epoch 30/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3797 - val_loss: 37817.8243\n",
            "Epoch 31/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3790 - val_loss: 28530.4016\n",
            "Epoch 32/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3790 - val_loss: 28844.6865\n",
            "Epoch 33/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3776 - val_loss: 25117.8496\n",
            "Epoch 34/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3757 - val_loss: 35330.4994\n",
            "Epoch 35/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3740 - val_loss: 28274.8817\n",
            "Epoch 36/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3731 - val_loss: 30049.9527\n",
            "Epoch 37/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3716 - val_loss: 34998.6108\n",
            "Epoch 38/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3706 - val_loss: 24876.5554\n",
            "Epoch 39/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3702 - val_loss: 29013.0484\n",
            "Epoch 40/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3718 - val_loss: 32230.3330\n",
            "Epoch 41/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3686 - val_loss: 33351.9843\n",
            "Epoch 42/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3683 - val_loss: 22464.4904\n",
            "Epoch 43/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3680 - val_loss: 26905.2810\n",
            "Epoch 44/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3681 - val_loss: 32358.1238\n",
            "Epoch 45/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3654 - val_loss: 33595.3403\n",
            "Epoch 46/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3638 - val_loss: 28367.9216\n",
            "Epoch 47/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3631 - val_loss: 23727.5305\n",
            "Epoch 48/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3620 - val_loss: 27342.8633\n",
            "Epoch 49/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3637 - val_loss: 25147.5812\n",
            "Epoch 50/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3615 - val_loss: 30040.2764\n",
            "Epoch 51/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3614 - val_loss: 24249.6297\n",
            "Epoch 52/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3586 - val_loss: 38353.0810\n",
            "Epoch 53/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3592 - val_loss: 19064.1297\n",
            "Epoch 54/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3573 - val_loss: 17285.6174\n",
            "Epoch 55/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3571 - val_loss: 21766.2808\n",
            "Epoch 56/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3566 - val_loss: 16852.4114\n",
            "Epoch 57/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3555 - val_loss: 23370.4195\n",
            "Epoch 58/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3545 - val_loss: 22237.4173\n",
            "Epoch 59/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3544 - val_loss: 19383.6785\n",
            "Epoch 60/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3542 - val_loss: 25439.3192\n",
            "Epoch 61/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3533 - val_loss: 21948.0493\n",
            "Epoch 62/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3528 - val_loss: 17102.8143\n",
            "Epoch 63/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3525 - val_loss: 20196.2047\n",
            "Epoch 64/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3537 - val_loss: 28569.0576\n",
            "Epoch 65/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3516 - val_loss: 19161.0470\n",
            "Epoch 66/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3502 - val_loss: 20886.9852\n",
            "Epoch 67/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3502 - val_loss: 20897.5643\n",
            "Epoch 68/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3500 - val_loss: 17664.5006\n",
            "Epoch 69/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3490 - val_loss: 16431.5058\n",
            "Epoch 70/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3497 - val_loss: 23617.6713\n",
            "Epoch 71/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3500 - val_loss: 24703.1793\n",
            "Epoch 72/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3488 - val_loss: 23565.2463\n",
            "Epoch 73/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3507 - val_loss: 17460.6480\n",
            "Epoch 74/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3478 - val_loss: 21953.6136\n",
            "Epoch 75/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3469 - val_loss: 21158.1971\n",
            "Epoch 76/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3460 - val_loss: 26332.5761\n",
            "Epoch 77/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3479 - val_loss: 19028.1643\n",
            "Epoch 78/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3459 - val_loss: 17391.8526\n",
            "Epoch 79/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3463 - val_loss: 25282.8484\n",
            "Epoch 80/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3453 - val_loss: 16703.5649\n",
            "Epoch 81/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3448 - val_loss: 19692.2775\n",
            "Epoch 82/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3439 - val_loss: 18702.2488\n",
            "Epoch 83/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3434 - val_loss: 21579.5169\n",
            "Epoch 84/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3441 - val_loss: 20162.0148\n",
            "Epoch 85/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3428 - val_loss: 19725.3520\n",
            "Epoch 86/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3439 - val_loss: 15260.0483\n",
            "Epoch 87/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3424 - val_loss: 21493.5844\n",
            "Epoch 88/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3426 - val_loss: 22472.1084\n",
            "Epoch 89/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3415 - val_loss: 20524.9860\n",
            "Epoch 90/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3423 - val_loss: 23739.2462\n",
            "Epoch 91/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3411 - val_loss: 29250.0724\n",
            "Epoch 92/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3419 - val_loss: 21495.0054\n",
            "Epoch 93/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3399 - val_loss: 22114.4750\n",
            "Epoch 94/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3402 - val_loss: 26927.3187\n",
            "Epoch 95/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3393 - val_loss: 21772.6825\n",
            "Epoch 96/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3391 - val_loss: 29627.6961\n",
            "Epoch 97/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3381 - val_loss: 20011.9391\n",
            "Epoch 98/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3392 - val_loss: 22721.0034\n",
            "Epoch 99/100\n",
            "11610/11610 [==============================] - 0s 32us/sample - loss: 0.3389 - val_loss: 20164.9513\n",
            "Epoch 100/100\n",
            "11610/11610 [==============================] - 0s 31us/sample - loss: 0.3370 - val_loss: 17014.7738\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f67a37a6940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjsS_4WIrMim",
        "colab_type": "code",
        "outputId": "b5c26fcb-e427-42dd-b0c6-326955c39f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3021
        }
      },
      "source": [
        "from scipy.stats import reciprocal\n",
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "param_distribs = {\n",
        "    \"n_hidden\": [0, 1, 2, 3],\n",
        "    \"n_neurons\": np.arange(1, 100),\n",
        "    \"learning_rate\": reciprocal(3e-4, 3e-2),\n",
        "}\n",
        "\n",
        "rnd_srch = RandomizedSearchCV(keras_reg, param_distribs, cv=3, n_iter=10, verbose=2)\n",
        "rnd_srch.fit(X_train, Y_train, validation_data=(X_val, Y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "[CV] learning_rate=0.0004926210035629582, n_hidden=1, n_neurons=85 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7740/7740 [==============================] - 0s 60us/sample - loss: 3.7693 - val_loss: 214601.7855\n",
            "3870/3870 [==============================] - 0s 22us/sample - loss: 1.9700\n",
            "[CV]  learning_rate=0.0004926210035629582, n_hidden=1, n_neurons=85, total=   0.9s\n",
            "[CV] learning_rate=0.0004926210035629582, n_hidden=1, n_neurons=85 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "7740/7740 [==============================] - 0s 60us/sample - loss: 3.6374 - val_loss: 163940.9959\n",
            "3870/3870 [==============================] - 0s 22us/sample - loss: 2.0009\n",
            "[CV]  learning_rate=0.0004926210035629582, n_hidden=1, n_neurons=85, total=   1.0s\n",
            "[CV] learning_rate=0.0004926210035629582, n_hidden=1, n_neurons=85 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 60us/sample - loss: 3.1686 - val_loss: 591042.9448\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 2.1903\n",
            "[CV]  learning_rate=0.0004926210035629582, n_hidden=1, n_neurons=85, total=   0.9s\n",
            "[CV] learning_rate=0.005915232327224446, n_hidden=3, n_neurons=65 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 1s 71us/sample - loss: 0.9888 - val_loss: 663.8837\n",
            "3870/3870 [==============================] - 0s 22us/sample - loss: 0.5574\n",
            "[CV]  learning_rate=0.005915232327224446, n_hidden=3, n_neurons=65, total=   1.0s\n",
            "[CV] learning_rate=0.005915232327224446, n_hidden=3, n_neurons=65 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 1s 77us/sample - loss: 0.9593 - val_loss: 61397.9896\n",
            "3870/3870 [==============================] - 0s 20us/sample - loss: 0.6924\n",
            "[CV]  learning_rate=0.005915232327224446, n_hidden=3, n_neurons=65, total=   1.0s\n",
            "[CV] learning_rate=0.005915232327224446, n_hidden=3, n_neurons=65 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 1.1258 - val_loss: 18163.0351\n",
            "3870/3870 [==============================] - 0s 20us/sample - loss: 0.5740\n",
            "[CV]  learning_rate=0.005915232327224446, n_hidden=3, n_neurons=65, total=   0.8s\n",
            "[CV] learning_rate=0.0009270932615340971, n_hidden=1, n_neurons=16 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 2.6525 - val_loss: 51775.1853\n",
            "3870/3870 [==============================] - 0s 17us/sample - loss: 1.0974\n",
            "[CV]  learning_rate=0.0009270932615340971, n_hidden=1, n_neurons=16, total=   0.7s\n",
            "[CV] learning_rate=0.0009270932615340971, n_hidden=1, n_neurons=16 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 2.0465 - val_loss: 1154177.9724\n",
            "3870/3870 [==============================] - 0s 17us/sample - loss: 1.5460\n",
            "[CV]  learning_rate=0.0009270932615340971, n_hidden=1, n_neurons=16, total=   0.7s\n",
            "[CV] learning_rate=0.0009270932615340971, n_hidden=1, n_neurons=16 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 46us/sample - loss: 3.9605 - val_loss: 31618.6204\n",
            "3870/3870 [==============================] - 0s 18us/sample - loss: 1.9910\n",
            "[CV]  learning_rate=0.0009270932615340971, n_hidden=1, n_neurons=16, total=   0.8s\n",
            "[CV] learning_rate=0.002449572492713086, n_hidden=3, n_neurons=76 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 56us/sample - loss: 2.2115 - val_loss: 10126.6635\n",
            "3870/3870 [==============================] - 0s 20us/sample - loss: 0.7510\n",
            "[CV]  learning_rate=0.002449572492713086, n_hidden=3, n_neurons=76, total=   0.8s\n",
            "[CV] learning_rate=0.002449572492713086, n_hidden=3, n_neurons=76 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 57us/sample - loss: 1.2664 - val_loss: 114934.5486\n",
            "3870/3870 [==============================] - 0s 20us/sample - loss: 0.6439\n",
            "[CV]  learning_rate=0.002449572492713086, n_hidden=3, n_neurons=76, total=   0.8s\n",
            "[CV] learning_rate=0.002449572492713086, n_hidden=3, n_neurons=76 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 1s 67us/sample - loss: 1.3848 - val_loss: 1935.7875\n",
            "3870/3870 [==============================] - 0s 25us/sample - loss: 0.8232\n",
            "[CV]  learning_rate=0.002449572492713086, n_hidden=3, n_neurons=76, total=   0.9s\n",
            "[CV] learning_rate=0.000509293557801316, n_hidden=3, n_neurons=28 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 1s 69us/sample - loss: 3.7209 - val_loss: 189407.5245\n",
            "3870/3870 [==============================] - 0s 27us/sample - loss: 1.9838\n",
            "[CV]  learning_rate=0.000509293557801316, n_hidden=3, n_neurons=28, total=   1.2s\n",
            "[CV] learning_rate=0.000509293557801316, n_hidden=3, n_neurons=28 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 1s 68us/sample - loss: 3.0707 - val_loss: 73462.6506\n",
            "3870/3870 [==============================] - 0s 26us/sample - loss: 1.8363\n",
            "[CV]  learning_rate=0.000509293557801316, n_hidden=3, n_neurons=28, total=   1.0s\n",
            "[CV] learning_rate=0.000509293557801316, n_hidden=3, n_neurons=28 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 1s 68us/sample - loss: 3.4564 - val_loss: 181528.3006\n",
            "3870/3870 [==============================] - 0s 23us/sample - loss: 2.1594\n",
            "[CV]  learning_rate=0.000509293557801316, n_hidden=3, n_neurons=28, total=   1.0s\n",
            "[CV] learning_rate=0.0019771459930714667, n_hidden=2, n_neurons=80 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 1s 69us/sample - loss: 1.5577 - val_loss: 244813.0349\n",
            "3870/3870 [==============================] - 0s 24us/sample - loss: 0.6920\n",
            "[CV]  learning_rate=0.0019771459930714667, n_hidden=2, n_neurons=80, total=   1.0s\n",
            "[CV] learning_rate=0.0019771459930714667, n_hidden=2, n_neurons=80 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 1s 83us/sample - loss: 1.4584 - val_loss: 446224.5706\n",
            "3870/3870 [==============================] - 0s 19us/sample - loss: 0.9038\n",
            "[CV]  learning_rate=0.0019771459930714667, n_hidden=2, n_neurons=80, total=   1.1s\n",
            "[CV] learning_rate=0.0019771459930714667, n_hidden=2, n_neurons=80 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 52us/sample - loss: 1.4004 - val_loss: 368345.9673\n",
            "3870/3870 [==============================] - 0s 18us/sample - loss: 0.7041\n",
            "[CV]  learning_rate=0.0019771459930714667, n_hidden=2, n_neurons=80, total=   0.8s\n",
            "[CV] learning_rate=0.002653546960530046, n_hidden=1, n_neurons=89 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 1.7175 - val_loss: 44368.4375\n",
            "3870/3870 [==============================] - 0s 17us/sample - loss: 0.7918\n",
            "[CV]  learning_rate=0.002653546960530046, n_hidden=1, n_neurons=89, total=   0.7s\n",
            "[CV] learning_rate=0.002653546960530046, n_hidden=1, n_neurons=89 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 48us/sample - loss: 1.5158 - val_loss: 299771.4974\n",
            "3870/3870 [==============================] - 0s 17us/sample - loss: 1.2119\n",
            "[CV]  learning_rate=0.002653546960530046, n_hidden=1, n_neurons=89, total=   0.7s\n",
            "[CV] learning_rate=0.002653546960530046, n_hidden=1, n_neurons=89 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 47us/sample - loss: 1.4785 - val_loss: 87101.3330\n",
            "3870/3870 [==============================] - 0s 17us/sample - loss: 0.6616\n",
            "[CV]  learning_rate=0.002653546960530046, n_hidden=1, n_neurons=89, total=   0.7s\n",
            "[CV] learning_rate=0.0025860412585500146, n_hidden=0, n_neurons=72 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 43us/sample - loss: 3.0190 - val_loss: 26953.8664\n",
            "3870/3870 [==============================] - 0s 17us/sample - loss: 0.9865\n",
            "[CV]  learning_rate=0.0025860412585500146, n_hidden=0, n_neurons=72, total=   0.6s\n",
            "[CV] learning_rate=0.0025860412585500146, n_hidden=0, n_neurons=72 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 1s 72us/sample - loss: 2.5315 - val_loss: 85097.7882\n",
            "3870/3870 [==============================] - 0s 16us/sample - loss: 2.1898\n",
            "[CV]  learning_rate=0.0025860412585500146, n_hidden=0, n_neurons=72, total=   0.8s\n",
            "[CV] learning_rate=0.0025860412585500146, n_hidden=0, n_neurons=72 ...\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 3.3035 - val_loss: 21432.5924\n",
            "3870/3870 [==============================] - 0s 18us/sample - loss: 1.1585\n",
            "[CV]  learning_rate=0.0025860412585500146, n_hidden=0, n_neurons=72, total=   0.6s\n",
            "[CV] learning_rate=0.004885356150379457, n_hidden=0, n_neurons=85 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 1.9042 - val_loss: 21229.4938\n",
            "3870/3870 [==============================] - 0s 16us/sample - loss: 0.7602\n",
            "[CV]  learning_rate=0.004885356150379457, n_hidden=0, n_neurons=85, total=   0.6s\n",
            "[CV] learning_rate=0.004885356150379457, n_hidden=0, n_neurons=85 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 44us/sample - loss: 1.9829 - val_loss: 17856.0433\n",
            "3870/3870 [==============================] - 0s 15us/sample - loss: 1.6054\n",
            "[CV]  learning_rate=0.004885356150379457, n_hidden=0, n_neurons=85, total=   0.6s\n",
            "[CV] learning_rate=0.004885356150379457, n_hidden=0, n_neurons=85 ....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 43us/sample - loss: 2.2694 - val_loss: 4007.3376\n",
            "3870/3870 [==============================] - 0s 16us/sample - loss: 0.8043\n",
            "[CV]  learning_rate=0.004885356150379457, n_hidden=0, n_neurons=85, total=   0.6s\n",
            "[CV] learning_rate=0.02234196348772313, n_hidden=3, n_neurons=31 .....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 53us/sample - loss: 1.5988 - val_loss: 1066.7622\n",
            "3870/3870 [==============================] - 0s 18us/sample - loss: 0.3890\n",
            "[CV]  learning_rate=0.02234196348772313, n_hidden=3, n_neurons=31, total=   0.8s\n",
            "[CV] learning_rate=0.02234196348772313, n_hidden=3, n_neurons=31 .....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 54us/sample - loss: 0.6886 - val_loss: 15666.2890\n",
            "3870/3870 [==============================] - 0s 18us/sample - loss: 0.5376\n",
            "[CV]  learning_rate=0.02234196348772313, n_hidden=3, n_neurons=31, total=   0.8s\n",
            "[CV] learning_rate=0.02234196348772313, n_hidden=3, n_neurons=31 .....\n",
            "Train on 7740 samples, validate on 3870 samples\n",
            "7740/7740 [==============================] - 0s 55us/sample - loss: 0.6565 - val_loss: 75051.2388\n",
            "3870/3870 [==============================] - 0s 18us/sample - loss: 0.4529\n",
            "[CV]  learning_rate=0.02234196348772313, n_hidden=3, n_neurons=31, total=   1.1s\n",
            "Train on 11610 samples, validate on 3870 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:   25.1s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "11610/11610 [==============================] - 1s 45us/sample - loss: 0.6788 - val_loss: 114015.1099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score='raise-deprecating',\n",
              "                   estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f67a3754128>,\n",
              "                   iid='warn', n_iter=10, n_jobs=None,\n",
              "                   param_distributions={'learning_rate': <scipy.stats._distn_infrastructure.rv_frozen object at 0x7f67b3542518>,\n",
              "                                        'n_hidden': [0, 1, 2, 3],\n",
              "                                        'n_neurons': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10,...\n",
              "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
              "       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n",
              "       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n",
              "       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n",
              "       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])},\n",
              "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f24z-eMg5vU2",
        "colab_type": "text"
      },
      "source": [
        "### Monte Carlo Dropout\n",
        "- Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning, Y. Gal and Z.\n",
        "Ghahramani (2016)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eYxZtlWmtoBb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.keras.backend.learning_phase_scope(1):\n",
        "    y_probas = np.stack([model.predict(X_test) for sample in range(100)])\n",
        "y_proba = y_probas.mean(axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}